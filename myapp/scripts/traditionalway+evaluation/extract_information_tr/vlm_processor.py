from transformers import BlipProcessor, BlipForConditionalGeneration
from PIL import Image
import numpy as np

class VLMProcessor:
    def __init__(self):
        # Initialize the VLM model and processor (e.g., BLIP)
        self.vlm_processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
        self.vlm_model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")

    def extract_vlm_info(self, image_path):
        """
        Extract contextual information from an image using VLM.

        :param image_path: Path to the image file
        :return: Descriptive text generated by VLM
        """
        image = Image.open(image_path).convert("RGB")
        
        image = np.array(image)  
        if image.shape[0] == 1:  
            image = np.transpose(image, (1, 2, 0)) 

        inputs = self.vlm_processor(images=image, return_tensors="pt")
        output = self.vlm_model.generate(**inputs)
        descriptive_text = self.vlm_processor.decode(output[0], skip_special_tokens=True)
        return descriptive_text
